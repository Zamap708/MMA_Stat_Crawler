from bs4 import BeautifulSoup
import requests
import json
from tqdm import tqdm

html_text_divisions_list = requests.get('https://www.fightmatrix.com/mma-ranks/').text
soup = BeautifulSoup(html_text_divisions_list, 'lxml')

table = soup.find('table')

divisions = table.find_all('a')

div_url_list = [] #Create empty array to store the division links
div_url_extra_pages = [] #Create empty array to store the pages generated by iterating through div_url_list

for division in divisions: #loop through all the division links on the page and add to the div url list array
    division_url = division.get('href') 
    div_url_list.append(division_url)

print(div_url_list)    
#Should have complete array that will automatically update if necessary

for div_url in div_url_list: #loops through every division url to get list of fighters
    html_text_division = requests.get(div_url).text

    soup2 = BeautifulSoup(html_text_division, 'lxml')
    
    page_nav = soup2.find('table', class_ = "pager")
    page_array_raw = page_nav.find_all('a')   
    
    if len(page_array_raw) == 2: #checking if there is more than 1 page in the division
        last_page_url = page_array_raw[1].get('href')
        last_page_index = last_page_url.find("=") + 1
        last_page = last_page_url[last_page_index:] #get last page number to use as maximum range         
        for i in range(2, int(last_page) + 1): #this for loop will generate links ranging from 2(Page 1 links are already available) till last page
            page_number_url = div_url[:len(div_url) - 1] + '?PageNum=' + str(i)
            div_url_extra_pages.append(page_number_url)

    #print(len(page_array_raw))


   
all_div_pages = div_url_list + div_url_extra_pages #combine original list with generated list resulting in array of every url

fighter_url_list = [] #empty array to store all fighter links
print("Collecting fighter links...")
for div_pages in tqdm(all_div_pages): #iterate through every page and generate a fighter link
    fighter_table = soup2.find('table', class_ = "tblRank")
    
    fighters = fighter_table.find_all('a', class_ = "sherLink")
    for fighter in fighters: #loop through all fighter hyperlinks and add them to list, concat with main domain to get complete url
        fighter_url = fighter.get('href')
        fighter_url_list.append('https://www.fightmatrix.com' + fighter_url)
 
print("Succesfully created fighter links!")   
    
# print(fighter_url_list)
print("Collecting fighter data...")
for fighter_pages in tqdm(fighter_url_list):
    
    url = fighter_pages
    
    fighter_html = requests.get(url).text
    soup = BeautifulSoup(fighter_html, 'lxml')

    content = soup.find('div', id="content")

    fighter_name = content.find('div', class_ = 'posttitle').text

    info_table = content.find('table', class_="tblRank")

    info_chart = info_table.find_all('div')

    rank_list = []

    bio_list = [] #empty list to store all info as single array, to be converted to JSON

    fighter_bio_dict = {'name' : fighter_name, 'url' : url}

    current_ranking = content.find('td', class_ = "tdRank").text
    ranking_array = current_ranking.split(':')

    for rank_data in ranking_array: #This organises rankings into usable JSON format
        rank_string = rank_data.replace('\n', '') 
        rank_list.append(rank_string)

    for info in info_chart:
        
        info_str = info.text.replace('\t', '')
        string_array = info_str.split(': ') #splits string into key:value array
        if len(string_array) == 2:
            
            key_value_pair = {string_array[0] : string_array[1]}
            bio_list.append(key_value_pair)
            
    bio_list.pop(0) #removes unnecessary header information
    if len(bio_list) > 13:
        bio_list.pop(13) #removes Highest quarterly rank

    rank_key_value_pair = {rank_list[0] : rank_list[1]}
    bio_list.append(rank_key_value_pair)

    for bio in bio_list: #Turns the list of objects into one object for JSON conversion
        fighter_bio_dict.update(bio)
    
    fighter_bio_json = json.dumps(fighter_bio_dict)
    
    url_splitter = url.split("https://www.fightmatrix.com/fighter-profile/")
    json_filename = url_splitter[1].replace('/', '').replace('+','_') + '.json'
    with open(json_filename, "w") as outfile:
        outfile.write(fighter_bio_json)
      
    #print(fighter_bio_json)

print("Fighter data succcesfully captured")